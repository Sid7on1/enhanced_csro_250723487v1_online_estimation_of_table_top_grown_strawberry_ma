{
  "agent_id": "coder4",
  "task_id": "task_4",
  "files": [
    {
      "name": "README.md",
      "purpose": "Project documentation",
      "priority": "medium"
    },
    {
      "name": "utils.py",
      "purpose": "Utility functions",
      "priority": "low"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.RO_2507.23487v1_Online_Estimation_of_Table_Top_Grown_Strawberry_Ma",
    "project_type": "computer_vision",
    "description": "Enhanced AI project based on cs.RO_2507.23487v1_Online-Estimation-of-Table-Top-Grown-Strawberry-Ma with content analysis. Detected project type: computer vision (confidence score: 12 matches).",
    "key_algorithms": [
      "Strawberry",
      "Gan",
      "Squares",
      "Pervised",
      "Enhance",
      "Hybrid-View",
      "Learning",
      "Contour",
      "Initial",
      "Machine"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.RO_2507.23487v1_Online-Estimation-of-Table-Top-Grown-Strawberry-Ma.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nOnline Estimation of Table-Top Grown Strawberry Mass in Field\nConditions with Occlusions\nJinshan Zhen\u20201,2, Yuanyue Ge\u20201, Tiaoxiao Zhu\u20201,3, Hui Zhao2and Ya Xiong1*\nAbstract \u2014 Accurate mass estimation of table-top grown\nstrawberries under field conditions remains challenging due\nto frequent occlusions and pose variations. This study proposes\na vision-based pipeline integrating RGB-D sensing and deep\nlearning to enable non-destructive, real-time and online mass\nestimation. The method employed YOLOv8-Seg for instance\nsegmentation, Cycle-consistent generative adversarial network\n(CycleGAN) for occluded region completion, and tilt-angle\ncorrection to refine frontal projection area calculations. A\npolynomial regression model then mapped the geometric fea-\ntures to mass. Experiments demonstrated mean mass estimation\nerrors of 8.11% for not-occluded strawberries and 10.47% for\noccluded cases. CycleGAN outperformed large mask inpaint-\ning (LaMa) model in occlusion recovery, achieving superior\npixel area ratios (PAR) (mean: 0.978 vs. 1.112) and higher\nintersection over union (IoU) scores (92.3% vs. 47.7% in the\n[0.9\u20131] range). This approach addresses critical limitations of\ntraditional methods, offering a robust solution for automated\nharvesting and yield monitoring with complex occlusion pat-\nterns.\nI. INTRODUCTION\nFruit mass estimation is essential for optimizing har-\nvest timing, improving agricultural efficiency, and advancing\nsmart, precision agriculture [1]. Additionally, fruit size and\nmass serve as key criteria for grading, aligning with the\nincreasingly diverse demands of the commercial market\n[2]. However, traditional electronic scale-based methods can\ndamage the delicate skins of fruits, leading to economic\nlosses, while manual grading requires substantial human\nlabor. Mass estimation for strawberries poses even greater\nchallenges, especially in strawberry-harvesting robots, where\nthe mass estimation must be conducted during the picking\nprocess, thus imposing higher demands on real-time per-\nformance and non-destructive operation. In field conditions,\nstrawberries are often occluded by stems, leaves, or other\nfruits, complicating fruit modeling and causing significant\ndeviations between the estimated and actual mass values [3].\nThis work was supported by the Haidian District Bureau of Agriculture\nand Rural Affairs, the Innovation Ability Project of Beijing Academy of\nAgricultural and Forestry Sciences (BAAFS) (KJCX20240321), the Out-\nstanding Youth Foundation of BAAFS (YKPY2025007), the Postdoctoral\nResearch Fund of BAAFS, the BAAFS Talent Recruitment Program, and the\nNSFC Excellent Young Scientists Fund (overseas). (* Corresponding Author:\nYa Xiong, yaxiong@nercita.org.cn )\n\u2020Jinshan Zhen, Yuanyue Ge and Tiaoxiao Zhu contributed equally to this\nwork and are co-first authors.\n1The Intelligent Equipment Research Center, Beijing Academy of Agri-\nculture and Forestry Sciences, Beijing 100097, China.\n2The College of Electrical Engineering and Automation, Tianjin Univer-\nsity of Technology, Tianjin, 300382, China.\n3The College of Mechatronic Engineering and Automation, Shanghai\nUniversity, Shanghai, 200444, China.Currently, the primary non-destructive measurement meth-\nods for fruit mass include near-infrared spectroscopy [4],\nlaser scanning [5], and computer vision image processing [6].\nIn general, computer vision methods offer significant advan-\ntages in terms of efficiency, multi-dimensional information\nacquisition, automation, and robotization, making them more\nwidely applicable and convenient.\nIn terms of shape recovery for fruits under occluded\nconditions, circular, elliptical, and other regular-shaped fruits\nare typically easier to reconstruct in terms of shape under\noccluded conditions. For example, in [7], an apple size\nestimation method combining an RGB-D camera with the\ngeometric shape of the apple was introduced. This model\nemployed binary masks and shape fitting algorithms to\naddress occlusion issues. Alternatively, [8] utilized Retanet\nto detect melons in images, applying the Chan-Vese active\ncontour algorithm and PCA ellipse fitting method to extract\ngeometric features from the detected cantaloupe images.\nA regression model was then developed based on these\ngeometric features, linking the elliptical characteristics (such\nas the major and minor axes) with melon mass to achieve\naccurate mass estimation.\nFor irregular-shaped fruits, such as strawberries, mass esti-\nmation is more challenging using traditional geometry-based\nshape recovery methods. Existing methods have primarily\nfocused on the mass estimation of not-occluded strawber-\nries. Tafuro et al. introduced a mass estimation method\nthat integrated RGB-D data, leveraging principal component\nanalysis (PCA) for dimensionality reduction and a Random\nForest model for mass prediction [9]. Similarly, Huang et\nal. developed a strawberry weight estimation method using\nplane-constrained binary division point cloud completion\n[10]. This approach required minimal data and achieves up to\na 20.95% improvement in prediction accuracy compared to\nstate-of-the-art methods. Additionally, Basak et al. explored\na strawberry weight estimation technique based on ma-\nchine learning, employing linear regression (LR) and support\nvector regression (SVR) models to predict weight while\nanalyzing the comparative advantages of both approaches\n[11]. In contrast to the above methods that only handle non-\noccluded strawberries, the method proposed in [12] addresses\ncalyx occlusion using right kite and simple kite geometry\nmodels, but it is limited in recovering or reconstructing other\nparts of the strawberry. Also, Ge et al. proposed a symmetry-\nplane-based method to complete the strawberry shape in 3D,\nbut it struggled to handle deformed point clouds from RGB-\nD cameras [13].\nGenerative Adversarial Networks (GANs) are deeparXiv:2507.23487v1  [cs.CV]  31 Jul 2025\n\n--- Page 2 ---\nFig. 1. Pipeline of mass estimation for occluded strawberries: (a) The RGB image is first processed by an instance segmentation network to isolate the\nstrawberry region, followed by occlusion restoration using CycleGAN; (b) The restored strawberry image is projected into a 3D space using a depth map\nand camera geometry for volume estimation, and the mass is subsequently predicted based on a volume-to-mass regression model derived from empirical\nstrawberry density measurements.\nlearning-based models consisting of a Generator and a Dis-\ncriminator, which are optimized through adversarial training\nto generate more realistic samples. GANs have been widely\napplied in image generation, style transformation, data aug-\nmentation, and image completion, achieving significant ad-\nvancements in these areas. Although several GAN-based\nimage completion methods have been proposed [14]\u2013[16],\nthey often suffer from training instability and are primarily\ndesigned for local inpainting, which limits their ability to\nreconstruct large missing regions.\nShapeMorph [17] realizes high-precision and diverse 3D\ncompletion with excellent detail fidelity through irregular\ncoding and chunked diffusion. However, it has two major\ndrawbacks: slow inference speed and the inability to seman-\ntically control the generated results. The diffusion model [18]\nperforms well in improving the accuracy and generalization\nability of 3D shape complementation, but still has two\nmajor drawbacks: first, the inference process is multi-step\niterative and has high computational overhead, which makes\nit difficult to meet real-time demands; second, the control\nability is limited, and there is a lack of explicit semantic\nregulation of the generated shapes, which makes it difficult\nto realize controllable generation and interactive editing.\nCycleGAN [19] offers notable advantages due to its unsu-\npervised learning capability, cross-domain image translation,\nand cycle consistency. Leveraging these strengths, we applied\nCycleGAN to recover occluded regions of strawberries,\nenabling mass estimation as if the fruit were fully visible.\nTo further enhance accuracy, we integrated the strawberry\nmass estimation pipeline with CycleGAN-based occlusion\nrecovery, thereby improving the robustness and adaptability\nof the method in complex field environments.\nOur contributions are as follows:\u2022We proposed an image restoration method to complete\nthe occluded parts of strawberries, providing an accurate\nsolution for strawberry mass estimation in complex field\nenvironments.\n\u2022An online visual strawberry mass estimation pipeline\nwas introduced, enabling direct mass determination\nusing an RGB-D camera that accounted for pose varia-\ntions.\nII. METHODS\nFig. 1 shows the entire pipeline of the proposed straw-\nberry mass estimation method. First, YOLOv8-Seg was used\nfor instance segmentation to achieve accurate separation of\nstrawberry pixels from the background. For the occlusion\ncase, CycleGAN adaptively reconstructed the occluded re-\ngion. Subsequently, the RGB image and depth information\nwere combined, and the projection angle was corrected to\naccurately obtain the cross-sectional area of the strawberry.\nFinally, volume and mass were calculated by regression\nanalysis to achieve high-precision mass prediction.\nA. Strawberry segmentation\nAlthough various instance segmentation models, such as\nMask R-CNN [20] and U-Net [21], have been developed,\nthey fail to meet the real-time processing demands of our\napplication. To address this limitation, we adopted YOLOv8-\nSeg, which offers superior real-time performance, as our\ninstance segmentation solution. YOLOv8-Seg leverages the\nenhanced CSPDarknet53 [22] as its backbone network and\nincorporates a Dynamic Mask Head to achieve precise, pixel-\nlevel segmentation. By building upon the strengths of its pre-\ndecessors, YOLOv8-Seg further enhanced instance segmen-\ntation by directly extracting both the Mask and the Region\nof Interest (RoI). This advancement significantly improved\n\n--- Page 3 ---\nsegmentation accuracy and efficiency while ensuring robust\nrecognition performance, even in the presence of overlapping\nor occluded objects.\nIn the data acquisition phase, we used an Intel RealSense\nD435 RGB-D camera to capture 1200 high-resolution straw-\nberry images from the strawberry tunnels. The dataset was\ndivided into training, testing, and validation sets in a 6:3:1\nratio, encompassing a variety of lighting conditions and\nvarying degrees of occlusion. This approach enhanced the\nmodel\u2019s generalization ability and ensured data diversity\nand complexity. Using the Labelme annotation platform, we\nmeticulously annotated the images with a polygonal segmen-\ntation tool to precisely outline the strawberry instances. The\nsamples were then classified into two categories: occluded\nand isolated. This categorization facilitated the subsequent\nrestoration process, where the shape of the strawberry could\nbe better understood and reconstructed.\nB. Shape completion for occluded strawberries\nCycleGAN is an unsupervised deep learning model for\nimage-to-image translation, capable of transforming images\nbetween different domains without requiring paired train-\ning data. In the task of strawberry occlusion recovery, we\nleveraged this capability by dividing the image into two\ndomains: one representing the occluded image and the other\nrepresenting the non-occluded image. These two domains\nwere derived from the output of the YOLOv8 instance\nsegmentation model.\nThe working schematic of CycleGAN is shown in Fig.2.\nSpecifically, the strawberry images were divided into oc-\nclusion domain A and completeness domain B. CycleGAN\ngenerated plausible content for the occluded regions through\nbi-directional mapping by converting the occluded image\n(domain A) into a completed image (domain B) and mapping\nit back from domain B to domain A, with a cycle consistency\nloss to ensure alignment with the original image in terms of\nvisual features and semantic structure. In the network struc-\nture, the generator G ABwas responsible for the conversion\nfrom occluded image to completed image, and the generator\nGBAreconstructed the occluded version from the completed\nimage. Both domain A (occluded images) and domain B\n(complete images) consist of real data naturally collected\nfrom field environments. The conversion from domain A\nto domain B is accomplished through adversarial training\nbetween the generator and discriminator, as defined in the\nCycleGAN framework.\nThe final optimization objective of CycleGAN was ob-\ntained by the weighted sum of the cycle consistency loss and\ntwo adversarial losses. Among them, the cycle consistency\nloss ensured that the image could be recovered to its original\nform after bi-directional conversion, thus maintaining struc-\ntural and content consistency. The adversarial loss matched\nthe distribution of the generated image to that of the real\nimage in the target domain, enhancing the realism of the\nconversion. The specific loss function formulas are shown\nbelow:\nFig. 2. CycleGAN-based shape completion framework: The generator\nGABtransforms an occluded strawberry image from domain A into a com-\npleted (fake) image in domain B, which is then passed to the discriminator\nDBfor adversarial training against real images in domain B. Meanwhile,\nthe reverse generator GBA reconstructs the original occluded image, and\nanL2loss is applied between the input and reconstructed image to enforce\ncycle consistency.\n1) In the CycleGAN structure, the cycle consistency loss\nis defined as follows:\nLcyc(G, F) =Ex\u223cpdata(x)[\u2225F(G(x))\u2212x\u22251]\n+Ey\u223cpdata(y)[\u2225G(F(y))\u2212y\u22251](1)\nwhere GandFrepresent the two generators. The sample\nxis first mapped to domain BbyG, then mapped back to\ndomain AbyF, ensuring it remains as close as possible\nto the original x.\u2225 \u00b7 \u2225 1represents the L1 loss, which is\nused to measure the pixel-wise difference before and after\ntransformation, helping to preserve the original information\nin the generated image.\n2) Two adversarial losses were employed for the unsu-\npervised mappings A\u2192B(viaG) and B\u2192A(viaF).\nTogether with the cycle consistency loss, they help ensure\nthe realism, stability, and reversibility of domain translation.\nThe two adversarial losses are defined as follows:\nLGAN(G, D Y, X, Y ) =Ey\u223cpdata(y)[logDY(y)]\n+Ex\u223cpdata(x)[log(1 \u2212DY(G(x)))] (2)\nLGAN(F, D X, Y, X ) =Ex\u223cpdata(x)[logDX(x)]\n+Ey\u223cpdata(y)[log(1 \u2212DX(F(y)))] (3)\nwhere xandydenote the occluded and non-occluded\nstrawberry images, respectively, and pdata(x)andpdata(y)\nrepresent their true distributions.\n3) The final optimization objective of CycleGAN is de-\nfined as the weighted sum of the adversarial losses and the\ncycle consistency loss:\nL(G, F, D X, DY) =LGAN(G, D Y, X, Y )\n+LGAN(F, D X, Y, X ) +\u03bbLcyc(G, F)(4)\n\n--- Page 4 ---\nwhere the hyperparameter \u03bbcontrols the weight of the\ncycle consistency loss in the total loss, balancing adversarial\nlearning and structural fidelity.\nTo enhance the diversity and robustness of the data, we\napplied data augmentation techniques, such as flipping, scal-\ning, and panning, to the images. During the training process,\nwe employed a staged strategy, dividing the generator\u2019s\ntraining into multiple phases, each focused on developing\ndifferent capabilities of the generator and gradually guiding\nit to generate more complex images. Simultaneously, we\nincreased the complexity of the discriminator in parallel\nwith the generator\u2019s progress, ensuring that the discriminator\ncould effectively evaluate the generator\u2019s output at each\nstage.\nAdditionally, we carefully tuned the training hyperparam-\neters: the batch size was set to 2, and the number of convo-\nlutional kernels for both the generator and the discriminator\nwas set to 96. The learning rate was set to 0.0002 for the\nfirst 100 epochs, then linearly decayed to 0.0001 for the\nsubsequent 100 epochs.\nThrough these training strategies, CycleGAN successfully\nlearned the mapping relationship between the two domains,\nenabling an efficient transformation from occlusion-broken\nimages to intact images.\nC. Volume regression considering strawberry poses and\nmass estimation\nThe visible area of the strawberry from the camera\u2019s\nperspective is usually smaller than the true frontal projected\narea because strawberries do not grow perfectly vertically.\nAs shown in (a) of Fig. 3, the visible area of a naturally\ngrown tilted strawberry is smaller than that of a vertically\noriented strawberry. This highlights the necessity of estimat-\ning the strawberry\u2019s pose. By leveraging the symmetry of the\nstrawberry\u2019s shape, we can obtain the tilt angle and correct\nthe projection error using cos\u03b8. Therefore, it is necessary\nto map the true frontal projection from the oblique angle,\nwhich requires estimating the pose of the strawberry.\nTo achieve this, we proposed a method for estimating the\ntilt angle based on the surface structure of the strawberry.\nBy analyzing the segmentation results, we first identified the\nstem and tip of the strawberry. Next, we computed the local\nconvex points and fitted a plane to these points, enabling the\ncalculation of the tilt angle relative to the vertical direction.\nIn the point cloud representation, the tilt angle of a\nstrawberry was calculated as follows: let Wrepresent the\npoint cloud set of the strawberry captured by the camera.\nThe point cloud subsets corresponding to the stem set ( S)\nand the tip set ( T) were utilized to determine the middle\nbelly point cloud set ( B). Beginning with an initial point ( p0)\nin the belly region, an iterative search radius was defined.\nWithin this radius ( r), the 30 nearest points from the subset\nWr={p0, . . . , p 29}were selected to construct a convex\nhull. The slope of this convex hull was computed, and the\ncurvature of the nearest convex hull points was iteratively\nevaluated. The point with the lowest curvature was identified\nFig. 3. Pose and projection of strawberries: (a) Comparison of\nthe projected visible areas between a vertically aligned strawberry and a\ntilted strawberry, highlighting how pose affects the observed contour. (b)\nIllustration of the tilt angle \u03b8formed between the strawberry\u2019s central axis\nand the vertical viewing direction, which is used for geometric correction\nin volume estimation.\nas the central point, representing the local convex apex ph\nof the strawberry\u2019s belly.\nTo estimate the tilt angle, this local convex point was\nconnected to a point within the tip point cloud. A random\nsample of 100 points from Wrwas selected, and a plane\nwas fitted using the least squares method. The tilt angle of\nthe fitted plane was then determined and used to compute\nthe tilt angle of the strawberry\u2019s central axis relative to the\nvertical direction. Finally, this angle was used to rectify the\ntilted strawberry\u2019s frontal projection, ensuring an accurate\nrepresentation.\nTo estimate strawberry mass, we first established a map-\nping between the RGB-D data and the fruit mass. After\ncalculating the frontal projection area of the strawberry, the\nnext step was to determine the relationship between the true\nprojection area and the volume. However, since the true\nfrontal projection area and volume did not generally exhibit a\ndirect linear relationship, polynomial regression was used to\nmodel this non-linear relationship. The formula is as follows:\ny=\u03b20+\u03b21x+\u03b22x2+\u03b23x3+\u00b7\u00b7\u00b7+\u03b2nxn+\u03f5(5)\nThis regression model, combined with the average density\nof ripe strawberries, enabled the final mass estimation used\nfor grading. This process served as the foundation for straw-\nberry mass estimation under ideal, occlusion-free conditions.\nThe primary source of deviation arose from the lack of\ncorresponding depth information in the RGB images restored\nby CycleGAN. To mitigate this issue, we supplemented\nthe missing depth data using the average depth of isolated\nstrawberries. In occluded scenarios, the CycleGAN network\nwas integrated to reconstruct the occluded portions of the\nstrawberry, enabling a more accurate mass estimation.\n\n--- Page 5 ---\nIII. R ESULTS AND DISCUSSION\nA. Results of strawberry segmentation\nYOLOv8-Seg was trained on an Nvidia RTX 4060 GPU\nusing the Adam optimizer with an initial learning rate of\n0.01. During training, the model gradually converged after\n100 epochs, ultimately achieving a mean average precision\n(mAP@0.5) of 0.91 on the validation set, as shown in Fig. 4.\nThe experimental results demonstrated that this method could\naccurately segment strawberry morphology and maintain\nhigh detection accuracy, even in the presence of complex\nbackgrounds and overlapping occlusions. This performance\nprovided strong technical support for subsequent morpholog-\nical recovery and strawberry mass estimation.\nFig. 4. Precision\u2013Recall (PR) curves for segmentation under occlusion:\nThe PR curves illustrate the segmentation performance of YOLOv8-Seg on\ncovered (orange), uncovered (light blue), and all samples (blue bold). The\nmodel achieved a mean average precision (mAP@0.5) of 0.910 across all\nclasses, with slightly higher performance on covered samples (0.915) than\nuncovered ones (0.905), demonstrating robustness against occlusions.\nB. Results of occlusion recovery model\nTo fully demonstrate the practicality and efficiency of the\nshape completion method, this study compared the Cycle-\nGAN model with the LaMa model, a deep learning approach\ndesigned for image restoration in large occluded areas. LaMa\nachieved a full image-wide receptive field by incorporating\nfast Fourier convolution (FFC), which enhanced its ability to\ncapture global information efficiently and delivered outstand-\ning performance in restoring large missing regions. For the\ncomparative analysis, we adopted three evaluation metrics:\npixel occupancy area, IoU, and inference time. Among these,\nthe pixel occupancy area was defined as the ratio of the\npixel area of the restored region to that of the original\nmissing region. The closer this value was to 1, the better\nthe restoration effect.\nThe comparison results are shown in Fig. 5. The upper\nplot shows the results of CycleGAN, while the lower plot\nshows the results of LaMa. The red dots indicate cases that\nfall outside the range of 1\u00b10.15. From the figure, it can be\nobserved that CycleGAN\u2019s results are overall denser, with\nfewer red markers, while LaMa\u2019s results are more scattered,with a higher number of red dots. This indicates that Cy-\ncleGAN achieves better recovery performance compared to\nLaMa.\nFig. 5. Scatter plots of pixel area ratios for CycleGAN and LaMa:\nEach dot represents the ratio between the pixel area of the restored fruit\nand the ground truth across 325 test images. The red dashed line denotes\nthe ideal ratio (1.00), while green dashed lines mark acceptable reference\nbounds (0.85 and 1.15). The top plot shows that CycleGAN exhibits a tighter\ndistribution around the ideal ratio, indicating better area consistency. In\ncontrast, the bottom plot shows that LaMa yields more outliers and higher\nvariance, suggesting reduced restoration stability.\nThe IoU metric was used to quantify the degree of overlap\nbetween the predicted contour and the true contour. It was\ncalculated as the ratio of the intersection area between the\npredicted and real contours to their union. Specifically, IoU\nvalues ranged from 0 to 1, where a value closer to 1 indicated\na higher degree of alignment between the predicted and\ntrue contours, signifying better restoration performance. As\nshown in Fig. 6, 92.3% of the test cases using CycleGAN\nachieve IoU values within the high-precision interval of\n[0.9\u20131], whereas only 47.7% of those using LaMa meet this\ncriterion. These results further demonstrate the efficiency and\nsuperiority of CycleGAN models in shape completion.\nFig. 6. IoU distribution comparison between CycleGAN and LaMa:\nPie charts visualize the distribution of IoU values for recovered strawberry\ncontours. The IoU range is divided into four intervals: [0.0, 0.6), [0.6, 0.8),\n[0.8, 0.9), and [0.9, 1.0]. A higher proportion in the [0.9, 1.0] interval\nindicates better shape recovery performance. CycleGAN achieves 92.3%\nin the highest interval, while LaMa reaches only 39.7%, confirming the\nsuperior contour alignment of CycleGAN.\n\n--- Page 6 ---\nTABLE I\nQUANTITATIVE PERFORMANCE COMPARISON BETWEEN CYCLE GAN\nAND LAMA. PAR DENOTES THE PIXEL AREA RATIO .\nModelMean\n(PAR)s2\n(PAR)Mean\n(IOU)s2\n(IOU)Inference Time\n(/frame)\nLaMa 1 + 0 .112 0.104 1\u22120.13 0.062 60 ms\nCycleGAN 1\u22120.022 0.022 1\u22120.045 0.083 200 ms\nTable I presents the detailed performance metrics for both\nmodels evaluated on 325 images, including the mean and\nvariance of the pixel occupancy ratio and IoU, as well\nas the average inference time per frame. The CycleGAN\nmodel took approximately 200 ms per frame when running\non a GTX 1060. When redeployed to an RTX 4060, the\ninference time improved to 110 ms, enabling near real-\ntime performance. The mean values indicated the deviation\nfrom the ideal value of 1, allowing for a more precise\ncomparison of the models\u2019 effectiveness. Overall, CycleGAN\noutperformed LaMa in both pixel occupancy ratio and IoU.\nHowever, it exhibited lower computational efficiency.\nTo visually compare the performance of CycleGAN and\nLaMa in occluded image recovery, Fig. 7 shows the compar-\native results of the overall contour reconstruction. CycleGAN\ndemonstrated a clear advantage in this regard. As shown in\nFig. 8, CycleGAN also performed well in texture restora-\ntion, effectively filling occluded regions while maintaining\nglobal image consistency. In contrast, the regions restored by\nLaMa often exhibited inconsistencies with the surrounding\ntextures. However, CycleGAN was constrained by its global\nreconstruction strategy, and its output resolution was fixed at\n256\u00d7256 pixels, which may limit the fidelity of fine details.\nFig. 7. Comparison of shape recovery performance: Visual comparison\nbetween CycleGAN and LaMa models in recovering strawberry contours\nunder occlusion. The leftmost column (a-1 to c-1) displays the original RGB\nimages captured in natural field environments. The second column (a-2 to c-\n2) shows the masked occluded inputs. The third column (a-3 to c-3) presents\nthe contour recovery results using CycleGAN, and the fourth column (a-4 to\nc-4) shows those obtained by LaMa. CycleGAN demonstrates better contour\nintegrity and symmetry in all cases, especially under large or irregular\nocclusions.\nFig. 8. Comparison of texture recovery performance: Visual results\nof occlusion restoration for three strawberries under different levels of\nocclusion. The left column (a-1 to c-1) shows masked inputs; the middle\ncolumn (a-2 to c-2) shows the texture recovery results using CycleGAN;\nand the right column (a-3 to c-3) shows results from LaMa. Compared to\nLaMa, CycleGAN produces more visually consistent and complete textures,\nespecially under large missing regions.\nC. Mass estimation results\nStrawberries, as irregular three-dimensional fruits, exhibit\na significant non-linear relationship between their surface\nprojected area and actual volume. This effect is particularly\npronounced in smaller fruits, where the volume is highly\nsensitive to changes in area, whereas volume growth lev-\nels off as the area increases. To better characterize this\ntrend, we employed a cubic polynomial regression model.\nCompared with linear or quadratic models, this approach\noffered stronger fitting capability and captured the non-linear\nrelationship between area and volume more effectively. The\nregression formula was derived from measured samples, and\nits coefficients reflected the empirical correlation between\narea and volume, enabling efficient estimation of strawberry\nvolume.\nTo assess the accuracy of the algorithm, a series of com-\nparative experiments were conducted. Field samples were\ncollected to record the maximum vertical cross-sectional\narea, tilt angle, volume, and mass of the strawberries. The\ncross-sectional area was measured manually through physical\nslicing; the tilt angle was obtained using a smartphone-\nbased angle measurement app; the volume was calculated\nvia Archimedes\u2019 water displacement method; and the mass\nwas measured with an electronic balance. The measured data\npoints and corresponding fitted curves are shown in Fig. 10.\nIn addition, we fitted a cubic polynomial regression model\nto describe the relationship between the maximum cross-\nsectional area and the volume of the strawberries:\ny=\u221224.9926 + 7 .1919\u00b7x\u22120.3063\u00b7x2+ 0.0052\u00b7x3(6)\n\n--- Page 7 ---\nAs shown in the figure and the regression equation above,\na strong correlation between cross-sectional area and volume\nwas observed, with a coefficient of determination R2=\n0.9037 , indicating a good model fit.\nFig. 9. Polynomial regression of surface area and volume: A third-order\npolynomial regression was used to fit the non-linear relationship between\nthe maximum cross-sectional area and the measured volume of strawberries.\nBlue dots represent actual measurements, and the red curve shows the fitted\npolynomial trend.\nTABLE II\nCOMPARISON OF PREDICTED AND ACTUAL CROSS -SECTIONAL AREA ,\nTILT ANGLE ,VOLUME ,AND MASS FOR ISOLATED STRAWBERRIES .\nArea Angle V olume Mass\nAverage error 4.14% 13.71% 7.52% 8.11%\nVariance 1.81 (cm2)22.79 (degree2)22.04 (cm3)21.67 (g)2\nThe evaluation results for isolated strawberries are sum-\nmarized in Table II. We compared the actual measured cross-\nsectional area, tilt angle, and volume of strawberries in\nthe field with the corresponding estimates produced by the\nproposed method. The mean deviation rate and variance were\nused as evaluation metrics.\nAs shown in the table, for isolated strawberries, the error\nin the cross-sectional area derived from depth and RGB\nimages remained small. In contrast, the tilt angle estimation\nexhibited larger errors due to the complexity and variability\nof field conditions; however, these errors were still within\nacceptable limits. The volume was estimated from the or-\nthogonal projected area using cubic polynomial regression,\nyielding an average error of 7.52%. Based on the average\nfruit density, the resulting mass estimation had a mean error\nof 8.11% and a variance of 1.67g2.\nFor occluded strawberries, as presented in Table III, the\nperformance of the CycleGAN-based recovery approach\nshowed only a slight increase in estimation error compared\nto the isolated cases. Although the CycleGAN reconstruction\nachieved high visual and geometric fidelity, minor deviations\nintroduced during the recovery process contributed to a\nmarginal rise in overall error. Nevertheless, the proposedmethod maintained strong robustness and reliable perfor-\nmance under occluded conditions.\nTABLE III\nPREDICTION ERRORS OF AREA ,VOLUME ,AND MASS FOR\nSHAPE -COMPLETED STRAWBERRIES .\nArea V olume Mass\nAverage error 6.38% 10.07% 10.47%\nVariance 2.91 (cm2)22.53 (cm3)24.76 (g)2\nIn addition to quantitative evaluation, we also conducted\nvisual verification of the proposed process in field conditions.\nAs shown in Figure 10: the top row displays original RGB\nimages of strawberries with varying degrees of occlusion; the\nbottom row shows the final output results, including mass\nestimation values and grading labels (mass >30 g is Grade\nA;(20,30]g is Grade B; (10,20]g is Grade C; \u226410 g\nis Grade D). This case demonstrated the process\u2019s ability\nto perform real-time detection, occlusion repair, and mass\nprediction under natural growth conditions.\nIV. DISCUSSION\nThis method was based on data collected by an RGB-D\ncamera, but under complex lighting conditions, depth maps\nwere susceptible to noise. To address this issue, multi-view\nor multi-modal data and adaptive repair strategies can be\ncombined in subsequent steps to improve geometric feature\nretention and estimation accuracy. During image restoration,\nthe input to the CycleGAN network was fixed at 256\u00d7256\npixels. Despite normalisation, some strawberry details and\ntextures may still be distorted, thereby affecting restoration\nmass and the accuracy of subsequent masss estimation. To\naddress this issue, future research could further enhance\nCycleGAN\u2019s detail restoration capabilities and improve the\naccuracy and robustness of mass prediction by optimising the\nnetwork structure, introducing higher-resolution inputs, or\nintegrating multi-scale feature fusion. Additionally, although\npolynomial regression was generally effective, it is highly\nsensitive to data distribution and may produce significant\nerrors when handling irregularly shaped strawberries. This\nmethod also assumed a fixed mapping relationship between\nmass and volume, without considering density changes\ncaused by factors such as cultivation variety, maturity, and\nmoisture content, which also affected the accuracy of mass\nestimation. To enhance prediction reliability, future research\ncould explore more robust nonlinear methods.\nV. CONCLUSIONS\nThis work presented a vision-based pipeline for on-\nline mass estimation of table-top grown strawberries under\nfield conditions, addressing occlusion challenges through\nCycleGAN-based shape completion and geometry correc-\ntion informed by pose estimation. By combining YOLOv8-\nSeg segmentation, unsupervised occlusion recovery, and tilt-\nadjusted projection analysis, the method achieved a mean\nmass estimation error below 10.5% for both isolated and\noccluded fruits. Key innovations included the adaptation\n\n--- Page 8 ---\nFig. 10. Test results under field conditions: Original RGB images of strawberries under varying occlusion and lighting conditions (top row), final\nsystem output showing estimated mass and grade in green text (bottom row).\nof CycleGAN for agricultural occlusion recovery and a\nsymmetry-based pose estimation algorithm.\nREFERENCES\n[1] Chiranjivi Neupane, Maisa Pereira, Anand Koirala, and Kerry B\nWalsh. Fruit sizing in orchard: A review from caliper to machine\nvision with deep learning. Sensors , 23(8):3868, 2023.\n[2] Naoshi Kondo. Automation on fruit and vegetable grading system and\nfood traceability. Trends in Food Science & Technology , 21(3):145\u2013\n152, 2010.\n[3] Ya Xiong, Yuanyue Ge, Lars Grimstad, and P \u02daal J From. An au-\ntonomous strawberry-harvesting robot: Design, development, integra-\ntion, and field evaluation. Journal of Field Robotics , 37(2):202\u2013224,\n2020.\n[4] Davide Cassanelli, Nicola Lenzini, Luca Ferrari, and Luigi Rovati.\nPartial least squares estimation of crop moisture and density by near-\ninfrared spectroscopy. IEEE Transactions on Instrumentation and\nMeasurement , 70:1\u201310, 2021.\n[5] Yuanshuo Hao, Faris Rafi Almay Widagdo, Xin Liu, Ying Quan, Lihu\nDong, and Fengri Li. Individual tree diameter estimation in small-scale\nforest inventory using uav laser scanning. Remote Sensing , 13(1):24,\n2020.\n[6] Hae-Il Yang, Sung-Gi Min, Ji-Hee Yang, Jong-Bang Eun, and Young-\nBae Chung. A novel hybrid-view technique for accurate mass\nestimation of kimchi cabbage using computer vision. Journal of Food\nEngineering , 378:112126, 2024.\n[7] Juan C Miranda, Jaume Arn \u00b4o, Jordi Gen \u00b4e-Mola, Jaume Lordan, Luis\nAs\u00b4\u0131n, and Eduard Gregorio. Assessing automatic data processing\nalgorithms for rgb-d cameras to predict fruit size and weight in apples.\nComputers and Electronics in Agriculture , 214:108302, 2023.\n[8] Aharon Kalantar, Yael Edan, Amit Gur, and Iftach Klapp. A deep\nlearning system for single and overall weight estimation of melons\nusing unmanned aerial vehicle images. Computers and Electronics in\nAgriculture , 178:105748, 2020.\n[9] Alessandra Tafuro, Adeayo Adewumi, Soran Parsa, Ghalamzan E\nAmir, and Bappaditya Debnath. Strawberry picking point localization\nripeness and weight estimation. In 2022 International conference on\nrobotics and automation (ICRA) , pages 2295\u20132302. Ieee, 2022.\n[10] Yanjiang Huang, Jiepeng Liu, and Xianmin Zhang. Strawberry weight\nestimation based on plane-constrained binary division point cloud\ncompletion. In 2024 IEEE International Conference on Robotics and\nAutomation (ICRA) , pages 11846\u201311852. IEEE, 2024.\n[11] Jayanta Kumar Basak, Bhola Paudel, Na Eun Kim, Nibas Chandra\nDeb, Bolappa Gamage Kaushalya Madhavi, and Hyeon Tae Kim. Non-\ndestructive estimation of fruit weight of strawberry using machine\nlearning models. Agronomy , 12(10):2487, 2022.\n[12] Lin Mar Oo and Nay Zar Aung. A simple and efficient method\nfor automatic strawberry shape and size estimation and classification.\nBiosystems engineering , 170:96\u2013107, 2018.[13] Yuanyue Ge, Ya Xiong, and P \u02daal J. From. Symmetry-based 3d shape\ncompletion for fruit localisation for harvesting robots. Biosystems\nEngineering , 197:188\u2013202, 2020.\n[14] Yongsheng Yu, Libo Zhang, Heng Fan, and Tiejian Luo. High-fidelity\nimage inpainting with gan inversion. In European Conference on\nComputer Vision , pages 242\u2013258. Springer, 2022.\n[15] Ankan Dash, Jingyi Gu, and Guiling Wang. Hi-gan: Hierarchical\ninpainting gan with auxiliary inputs for combined rgb and depth\ninpainting. arXiv preprint arXiv:2402.10334 , 2024.\n[16] Andranik Sargsyan, Shant Navasardyan, Xingqian Xu, and Humphrey\nShi. Mi-gan: A simple baseline for image inpainting on mobile\ndevices. In Proceedings of the IEEE/CVF International Conference\non Computer Vision , pages 7335\u20137345, 2023.\n[17] Jiahui Li, Pourya Shamsolmoali, Yue Lu, and Masoumeh Zareapoor.\nShapemorph: 3d shape completion via blockwise discrete diffusion.\npages 2818\u20132827, 2025.\n[18] Ruihang Chu, Enze Xie, Shentong Mo, Zhenguo Li, Matthias Nie\u00dfner,\nChi-Wing Fu, and Jiaya Jia. Diffcomplete: Diffusion-based generative\n3d shape completion. 2023.\n[19] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Un-\npaired image-to-image translation using cycle-consistent adversarial\nnetworks. In Proceedings of the IEEE international conference on\ncomputer vision , pages 2223\u20132232, 2017.\n[20] Kaiming He, Georgia Gkioxari, Piotr Doll \u00b4ar, and Ross Girshick.\nMask r-cnn. In Proceedings of the IEEE international conference\non computer vision , pages 2961\u20132969, 2017.\n[21] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Con-\nvolutional networks for biomedical image segmentation. In Medical\nimage computing and computer-assisted intervention\u2013MICCAI 2015:\n18th international conference, Munich, Germany, October 5-9, 2015,\nproceedings, part III 18 , pages 234\u2013241. Springer, 2015.\n[22] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu, Ping-Yang\nChen, Jun-Wei Hsieh, and I-Hau Yeh. Cspnet: A new backbone\nthat can enhance learning capability of cnn. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition\nworkshops , pages 390\u2013391, 2020.",
  "project_dir": "artifacts/projects/enhanced_cs.RO_2507.23487v1_Online_Estimation_of_Table_Top_Grown_Strawberry_Ma",
  "communication_dir": "artifacts/projects/enhanced_cs.RO_2507.23487v1_Online_Estimation_of_Table_Top_Grown_Strawberry_Ma/.agent_comm",
  "assigned_at": "2025-08-03T21:06:28.326914",
  "status": "assigned"
}